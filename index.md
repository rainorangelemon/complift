---
layout: default
name: "Improving Compositional Generation with Diffusion Models Using Lift Scores"
title:  "Improving Compositional Generation with Diffusion Models Using Lift Scores"
description: a compositional criteria for rejection and resampling of samples generated by diffusion models
date:   2020-05-21 15:38:11 -0800
usemathjax: true
---

Diffusion models have revolutionized generative modeling, particularly in producing high-quality images. Yet, when faced with complex prompts involving multiple objects or conditions, they often stumble: generated images might satisfy only part of a prompt or contain inconsistencies.

In our ICML 2025 paper, we introduce **CompLift**—a lightweight, training-free rejection criterion using **lift scores**—to address this issue.

# Table of Contents:

1. [Why Do Compositional Prompts Break Diffusion Models?](#-why-do-compositional-prompts-break-diffusion-models)
2. [What is CompLift?](#-what-is-complift)
3. [Lift Scores, Explained](#-lift-scores-explained)
4. [Compositional Logic via Lift](#-compositional-logic-via-lift)
5. [Efficient Implementation](#-efficient-implementation)
6. [Results](#-results)
7. [Lift in the Latent Space](#-lift-in-the-latent-space)

## 🤔 Why Do Compositional Prompts Break Diffusion Models?

Compositional prompts require satisfying multiple conditions simultaneously. Some diffusion models, e.g., Stable Diffusion, trained on rich datasets tend to focus on the objects mentioned with an earlier order in the prompt. As a result, given a prompt like:

> "a black car and a white clock"

The model might tend to generate a black car, and ignore the white clock. See the example below - here we have 100 samples generated by a diffusion model, and the first 62 samples are blue-bordered, indicating they satisfy the constraint to include a white clock, while the last 38 samples are orange-bordered, indicating they do not include a white clock.

<style>
.grid-container {
  display: grid;
  grid-template-columns: repeat(10, 1fr);
  gap: 5px;
  max-width: 800px;
  margin: 20px auto;
  position: relative;
}

.grid-item {
  aspect-ratio: 1;
  background-color: #f0f0f0;
  border: 2px solid #ddd;
  border-radius: 4px;
  transition: all 0.3s ease;
  position: relative;
  background-size: cover;
  background-position: center;
  cursor: pointer;
}

{% assign labels = "0,1,0,0,1,0,0,1,0,1,1,1,1,1,1,1,1,0,0,0,1,0,1,1,0,0,0,1,1,1,0,1,1,0,1,1,1,0,0,1,1,0,1,1,1,0,1,0,1,0,1,0,1,0,0,1,1,0,0,1,0,1,1,0,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,0,1,0,0,1,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1,1" | split: "," %}

{% for i in (0..99) %}
{% assign label = labels[i] %}
{% if label == "1" %}
.grid-item:nth-child({{ i | plus: 1 }}) {
  border-color: #2196F3;
}
{% else %}
.grid-item:nth-child({{ i | plus: 1 }}) {
  border-color: #FF9800;
}
{% endif %}
{% endfor %}

.grid-item:hover {
  transform: scale(4);
  z-index: 2;
  box-shadow: 0 4px 15px rgba(0,0,0,0.2);
  border-radius: 8px;
  border-width: 3px;
}

/* Adjust transform origin based on position */
.grid-item:nth-child(10n) {
  transform-origin: left center;
}
.grid-item:nth-child(10n+1) {
  transform-origin: right center;
}
.grid-item:nth-child(-n+10) {
  transform-origin: center bottom;
}
.grid-item:nth-child(n+91) {
  transform-origin: center top;
}

/* Generate background-image rules for all 100 items */
{% for i in (0..99) %}
.grid-item:nth-child({{ i | plus: 1 }}) {
  background-image: url('{{ site.baseurl }}/images/{{ i }}.jpg');
}
{% endfor %}
</style>

<div class="grid-container">
  {% for i in (0..99) %}
    {% assign label = labels[i] %}
    <div class="grid-item" data-label="{{ label }}"></div>
  {% endfor %}
</div>

## 🚀 What is CompLift?

**CompLift** is a plug-and-play resampling technique that checks if a generated sample truly satisfies a prompt—**without retraining or using any extra models**. It leverages a classic statistical idea: **lift**.

### 🔍 Lift Scores, Explained

Lift measures how much a condition $\vc$ influences the probability of generating a sample $\vx$. Formally:

$$\text{Lift}(\vx|\vc) = \log\left( \frac{p(\vx|\vc)}{p(\vx)} \right)$$

We approximate this using the internal denoising predictions of the diffusion model itself. Intuitively:

> If a sample aligns with the condition, the model should be **better at denoising it** when given that condition.

This allows us to check conditions *after* generation, and reject samples that don't satisfy them.

## 🧠 Compositional Logic via Lift

CompLift handles logical combinations of conditions:

- **AND (Product)**: Accept only if all subconditions have positive lift.
- **OR (Mixture)**: Accept if *any* subcondition has positive lift.
- **NOT (Negation)**: Reject if a subcondition is satisfied.

This compositional approach lets us flexibly combine prompts into logical structures—just like symbolic logic!

## ⚙️ Efficient Implementation

We propose two versions:

- **Naive CompLift**: Uses multiple sampling trials and computes lift via Monte Carlo.
- **Cached CompLift**: Reuses model outputs during sampling for **zero extra compute** in many settings.

## 🧪 Results

We evaluate CompLift on:

- 🎨 2D synthetic tasks
- 🧱 CLEVR object positioning
- 🖼 Text-to-image generation (SD 1.4/2.1/XL)

Across the board, CompLift significantly improves prompt alignment without harming quality.

| Task           | Metric                | Baseline | +CompLift |
|----------------|------------------------|----------|-----------|
| 2D Product     | Accuracy               | 74.3%    | **94.8%** |
| CLEVR (5 objs) | All-Cond Satisfaction  | 49.2%    | **76.4%** |
| SD 2.1         | CLIP alignment (avg.)  | 0.287    | **0.345** |

## 🔬 Lift in the Latent Space

To handle fine-grained image prompts, we compute lift *per pixel* in the latent space, allowing us to detect whether each object is truly present. This even helps in understanding *which* part of the image aligns with each prompt component.

## 🧰 Get Started

Want to try it out?

```bash
git clone https://github.com/rainorangelemon/complift
cd complift
python run_lift.py --prompt "a black car and a white clock"
```

## 🧩 Final Thoughts

CompLift shows how diffusion models can be made **compositional**—by introspecting their own denoising behavior. It's simple, elegant, and practical.

While we've focused on image generation, the principles behind lift scores may apply to **video, music, or even language generation** in the future.

## 📄 Paper

> Yu, C., & Gao, S. (2025). *Improving Compositional Generation with Diffusion Models Using Lift Scores*. ICML 2025. [[GitHub](https://github.com/rainorangelemon/complift)]